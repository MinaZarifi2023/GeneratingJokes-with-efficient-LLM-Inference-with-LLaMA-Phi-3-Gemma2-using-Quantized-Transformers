# GeneratingJokes-with-efficient-LLM-Inference-with-LLaMA-Phi-3-Gemma2-using-Quantized-Transformers
This notebook demonstrates how to load, configure, and run inference with multiple popular instruction-tuned LLMs — including LLaMA 3.1, Phi-3, and Gemma-2 — using Hugging Face Transformers and low-bit quantization for efficient inference on consumer GPUs or Google Colab.
