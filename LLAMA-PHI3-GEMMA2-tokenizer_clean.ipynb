{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKs1PM-O-VQa"
   },
   "source": [
    "This notebook can run on a low-cost or free T4 runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "EhKhPOQqR2gB",
    "outputId": "4a856280-f609-4ce0-9200-6865460bc0d3"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'LLAMA-PHI3-GEMMA2-tokenizer.ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3-1743757880.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LLAMA-PHI3-GEMMA2-tokenizer.ipynb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"widgets\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metadata\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"widgets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'LLAMA-PHI3-GEMMA2-tokenizer.ipynb'"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "nb = nbformat.read(\"LLAMA-PHI3-GEMMA2-tokenizer.ipynb\", as_version=4)\n",
    "if \"widgets\" in nb[\"metadata\"]:\n",
    "    del nb[\"metadata\"][\"widgets\"]\n",
    "with open(\"LLAMA-PHI3-GEMMA2-tokenizer_clean.ipynb\", \"w\") as f:\n",
    "    nbformat.write(nb, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NthhKJRwX1iS",
    "outputId": "b0b57fd7-9b3a-4858-e51f-01bea4267d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m440.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C9zvDGWD5pKp"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyKWKWSw7Iqp"
   },
   "source": [
    "# Sign in to Hugging Face\n",
    "\n",
    "1. create a free HuggingFace account at https://huggingface.co and navigate to Settings, then Create a new API token, giving yourself write permissions by clicking on the WRITE tab\n",
    "\n",
    "2. Press the \"key\" icon on the side panel to the left, and add a new secret:\n",
    "`HF_TOKEN = your_token`\n",
    "\n",
    "3. Execute the cell below to log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xd7cEDUC6Lkq"
   },
   "outputs": [],
   "source": [
    "hf_token = userdata.get('my_first_token')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UtN7OKILQato"
   },
   "outputs": [],
   "source": [
    "# instruct models\n",
    "\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA2 = \"google/gemma-2-2b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KgxCLBJIT5Hx"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSiYqPn87msu"
   },
   "source": [
    "# Accessing Llama 3.1 from Meta\n",
    "\n",
    "In order to use the fantastic Llama 3.1, Meta does require you to sign their terms of service.\n",
    "\n",
    "model instructions page in Hugging Face:\n",
    "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\n",
    "\n",
    "approval comes in a couple of minutes. Once you've been approved for any 3.1 model, it applies to the whole family of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hhOgL1p_T6-b"
   },
   "outputs": [],
   "source": [
    "# Quantization Config - this allows us to load the model into memory and use less memory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "153434da5dab480fa64afb5295417b49",
      "fb7af2913f8d4019843993153cfb499d",
      "cdaf9b223c8441a898f05398334bf44c",
      "3e7d3bf1698c4fb79dcc7f6da2bcb6b4",
      "ce1d7cb3ab3c49bb8bda0cde7cc5266c",
      "418bdfad7240409eb801b629d4d17a37",
      "dc519e038ea24b7d8b3162da4c73e965",
      "dd44c46ef2fb4d2f813fd11db837e3bb",
      "70ed7821a6e8442085bb9d674b15d534",
      "c3a19e9c838744b98dc12e88713ce6a7",
      "8a463a10c47d4a99b8c1f453945161b4",
      "387f1cc666964bdfaad212b03141f5c1",
      "75c399e8108b4df7a8d7243ee07bfa2e",
      "aafab449bce94750850a189c05a437ba",
      "83b8c0773a3a459caac79b46b8de6930",
      "dc7245aac5d94f9db88663f9f4498357",
      "cfa1dc01d870448bbd73a0799ade26e1",
      "493faf36a29c4b66930563d283da6d25",
      "658ccf0b2624495f9302483af678b14c",
      "2918fe8465e649f2b2a7246b5e353b90",
      "d08324dbeee34142a04e82faf3d0357e",
      "e7793ded529b471dade728675ce9d1dc",
      "5ec170299ef64339a56e28ccb5e77d82",
      "72f6c1a4bded438c9ee66faeea6f4da4",
      "47f9166cb9d443cd88fec575effc4a1a",
      "3e6c70f62e68457fb3f8727557819d63",
      "9bc1547f41884eebb24e2dd7a07afc88",
      "fa2017ef91ea4407bcc021e8ae12e102",
      "32dd4ea702034913883287985aa94694",
      "ae40e9291c084892a291fefebf3edb04",
      "fa377c7baacb4917ba2ab0889cc8b86e",
      "6da71e0077cc4cd798ee6cc1f33fbb21",
      "5b742c4516834ecfa967121308f4caed"
     ]
    },
    "id": "Zi8YXiwJHF59",
    "outputId": "21ce2faa-2e3a-4a07-d720-562f647e019a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153434da5dab480fa64afb5295417b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387f1cc666964bdfaad212b03141f5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec170299ef64339a56e28ccb5e77d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417,
     "referenced_widgets": [
      "80f931a611344046a8eeb6828bd0ab21",
      "f55961d269534f3a900a7ab7bfc049d5",
      "53d899f678a54ff5a01e29dcfc5b5095",
      "3274047de08844449e67c16eaab21e05",
      "0bfce03b6c5f4cf689aed176ed1a57a4",
      "5b51ab8360c54f5b99b880cdfb957ec2",
      "78b5b6e8b439458f81d3f06c3fa4b9e6",
      "597c5daff9bb44acba9eafc4b4995859",
      "bb21e62187be42f9bab9ae82e912d51f",
      "5d1717019c6644f5a4b44179c26be565",
      "10522d75c1d5418c895be41dc84c8b46",
      "091f336f5dcd487dbd166df792ca6454",
      "d6a23773ba2946638b99a95295ce2d92",
      "66bdaadafdd44aee9f4b94315f626fb9",
      "3fdf3bf0ba91401685f367def6ffda32",
      "ceae75914ef8441ba5c9f358e3bb4688",
      "405b7a22cd0a4c89b8cb563e094edf33",
      "9909e9527a4e4653a4885557b930e23f",
      "26cead47255b4b1d861e7ac4474413e7",
      "25ca260a6fb34e66ab9326dcb44ff0dd",
      "38cb294357454138918ecb36bcc3b7d0",
      "50be6ac2a7e240afb7a55bef3b4d3540",
      "0c974a6559d9432f917894f7efa43b76",
      "ccbb8ec2486146beac89571122268262",
      "f864c1c1fdc144efa911b4e9ba1088d2",
      "869860c7e9b8421784d03afb3ee3e5fe",
      "b88b53eae7da48f98eb66843a6d3016d",
      "aeb09c8b9ec74a0c8ef6c694a9664d29",
      "c93916f276c44fc899f1db9de1a98b94",
      "463f8470b38b4f9b88a73cfd5226a779",
      "5bbd544911854c7298b75f60135293e1",
      "e18c3c9ca72c48f9ab6e0b61cb0fd997",
      "c8612c54f3034eab822ddb5c7b522465",
      "797baf7272bd490985a6298ac4cdf111",
      "117e7d2c36704b8c97da989cef2ce8e5",
      "915b67ca16d24b388e5836237ef309b1",
      "22824a117c37447c8debaee950cd5431",
      "ee49bdece069470681146f1c37a281be",
      "3270d4fab98a4f3c9d1c2675434071dc",
      "8900b6aa0b4f4712a38e952a46536574",
      "04cb3a1fefb84a51a185ae07efe29fab",
      "49a9f2a972cc43c48574a4126adcbae5",
      "17e3e9630bbf4cffb3a085a31610151e",
      "3dad66919ddd47749f97978c46046f6b",
      "369f9d46a4e7434090b6fd10cd046e98",
      "113059e929eb481394ca1795a43b5009",
      "6352475c2fef4d06af75ee330cec1e7c",
      "d1c5fa9c7ec64799998629c04c4103a8",
      "3076a6061bca47e4af519c936a4c023f",
      "b0ae671efd9843719da480288dad3dfd",
      "a122cd0a150047249551a426d61800d5",
      "1dbaa76345854c1eaae2767ccdc78663",
      "ecb35311167244f6901b5819c0c0e8ee",
      "c0c243b20207409aae3d8dbf70bf350a",
      "fd49c10b13504f11ad5d0fc27ad96999",
      "7ed440865e854ec68bf3c74a1ca0dd9f",
      "4099183923af4b0e96791714c5fc9271",
      "09548e79f2cf4a9c8c62a38d1aff96ca",
      "aa52a8a504c6455e878a0745cd4fc5f8",
      "e187dab501234ef58406a2508d22f5a0",
      "cf2bc372d89e43dc914dc828b4103ac3",
      "1e319fcd065c4d15b20e103f36977af8",
      "3d4894e78d9045528cab4c0819bb6150",
      "3576bcdd25e34632af081473f59c674a",
      "eed558d2e7734500b700867e0c805837",
      "1adb332908fc42fdba2879918aa56554",
      "0d0ab3976c254b4798a1ca73055f8e8c",
      "685ed18622264dda8a196c91ee4ba0a9",
      "d0adf22b565b409cacc1d86200e561dd",
      "2973f8fc76854590bf1e0b5f04d90cde",
      "624f0ff160434b6b8deab4804f0392fd",
      "36d9cbc835864cb5bd72f51fe92e953a",
      "8a2ebadcdd1848ca93f2a28d77547e48",
      "3fd7a4a1c94f4d77b446a31c09c13c8b",
      "6d28ca32558f41ff9604a613ecd6f77d",
      "59ce1c7f520c42eea134c73fd9cf3512",
      "d12765300674489ea62e85b1deb41c99",
      "15e663bc9e7c403abd7ab1c36394a0c6",
      "588ac7bfcd2f445d89b2a0aa6b1c91db",
      "57dcc255597f47dfbe6967d28b46aa86",
      "3fbf7b7be68f47f197c3729d03aa77ed",
      "c394d8e787f6498ba397722c6fc2dee1",
      "0783d3c7ab814862a4eb319f63fc4224",
      "197c7d4f62704f98838ac810974dc189",
      "d0cb85bf4ec8448ab769451f127cb057",
      "3bcb68f430f24668afc82037050e3b1a",
      "ca3320e059dc48bba2ebd57d01bea017",
      "6a0554a6322a4ef68cdb6990cea5da1f",
      "ceabe7a8c97646659f702953a9b9fb4a",
      "16061b63bd59485d974d0b9848c2abc9",
      "53ab066bbdfa489c89ca0d6a19da79e9",
      "88d35a5e230e4fd1a99419e052296f1b",
      "0f72213013414163bf79b3baa19f6cfc",
      "cadc4c5fd68f4f4e81714b3b783f7afd",
      "629b47beadb44b70b58362e28a733eda",
      "c136e55420c346e3a5c3ee933353f463",
      "341b5020a44d4499b728f2432da71780",
      "b21b7f7f91ac40fb96fdcf37de2883f6",
      "cb37f0ac66a548a185b0fa7957e88f72"
     ]
    },
    "id": "S5jly421tno3",
    "outputId": "46c693b8-a6b7-4399-8018-fdf3d788c7c6"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f931a611344046a8eeb6828bd0ab21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091f336f5dcd487dbd166df792ca6454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c974a6559d9432f917894f7efa43b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797baf7272bd490985a6298ac4cdf111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369f9d46a4e7434090b6fd10cd046e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed440865e854ec68bf3c74a1ca0dd9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0ab3976c254b4798a1ca73055f8e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e663bc9e7c403abd7ab1c36394a0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceabe7a8c97646659f702953a9b9fb4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdbYaT8hWXWE",
    "outputId": "5a66f249-e798-4a5e-cfea-ba5b45429bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint: 5,591.5 MB\n"
     ]
    }
   ],
   "source": [
    "memory = model.get_memory_footprint() / 1e6\n",
    "print(f\"Memory footprint: {memory:,.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P0qmAD5ZtqWA",
    "outputId": "f9d14986-41e7-4c25-bc34-ddb7d90609dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkYEXzbotcud",
    "outputId": "0672e53f-47fe-4631-fe15-dc232fe4eea9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Why did the logistic regression model go to therapy?\n",
      "\n",
      "Because it was struggling to classify its emotions.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Run the model!\n",
    "\n",
    "outputs = model.generate(inputs, max_new_tokens=80)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oL0RWU2ttZf"
   },
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "\n",
    "del model, inputs, tokenizer, outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDCeJ20e4Hxx"
   },
   "source": [
    "I'm using a HuggingFace utility called TextStreamer so that results stream back.\n",
    "To stream results, we simply replace:  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80)`  \n",
    "With:  \n",
    "`streamer = TextStreamer(tokenizer)`  \n",
    "`outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)`\n",
    "\n",
    "Also I've added the argument `add_generation_prompt=True` to my call to create the Chat template. This ensures that Phi generates a response to the question, instead of just predicting how the user prompt continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO_VYZ3DZ7cs"
   },
   "outputs": [],
   "source": [
    "# Wrapping everything in a function - and adding Streaming and generation prompts\n",
    "\n",
    "def generate(model, messages):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "  del model, inputs, tokenizer, outputs, streamer\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "ae032f7a014a406ab2581f795c6cc70d",
      "34cfb5bdf621410d8c2686bd2721c9ab",
      "62e7a095214f49c0aa67e28872b6f758",
      "a83c8ae3a91d4ee388a0bb4457a00eb5",
      "af34433de853490cb33e85b1a84cb90e",
      "9e2715f363ae4d0793e476832bcb99c0",
      "880f9a1c36b444b081282149f3d6456e",
      "9498e07e5552489995efcb5221d95bda",
      "8151d914fa0541519c6be29de92edb09",
      "08e466b649974ab9ba9281df8829313c",
      "696fa98cf9b04fc78af720c8bdca3dfc",
      "457331b99edc45929a5d078293f22cf2",
      "69ccf3f0219c4e63bd9fcdbf5da8e563",
      "d64fba2f044741d4bb86031bc9cf5880",
      "89460d28783e45528ea179e57b886344",
      "86816f0a63b34785b6afdf0e3b9c10e6",
      "e84f7f2aa4db4e23be4cd1522f5fa93e",
      "5d9592fe00fd44dc8b879715fd5dbba2",
      "b5a3f69469e14d79ba3d3b556f680dc6",
      "28a548c2596144ab8241ff66fe92cb97",
      "477bbfdc7f0549a19bf46ddc57d01a92",
      "9a5be98d624d48dcaa9912f68b5fb82c",
      "e249592783a94ba1ab68a4aa3c6fc9b3",
      "f4b77b0be75f4167b9a3c8771ac84006",
      "993702ec3dd449fb82827617a1c88abf",
      "f57cd26b293746798b39a11e1eb7006d",
      "b53603881b72456e8777d7b9bbf05dd2",
      "83cae66c846745ac8d242bc9c268501a",
      "e453e27b81d74d7b9b977aded929e731",
      "ca77085c325a4648b4a72818e849b00a",
      "a46982fc8f8749a3b1e91eaa2a0f66e6",
      "492b3e30c3174d2193b1da4de2589cc2",
      "431b373be9224b04be8eac190a362bd0",
      "0f6c62f850cf4d0a941af1f69fbb5e5f",
      "bf1c1914e3b94f619e3ed29e7fc542d7",
      "a5d349f5b2964208b0a7b72bc214585c",
      "68aff936248d4f5cb986ad5d3ee903bc",
      "4e9bf8c85cb34e95ae40f5647453896c",
      "b3cfe68a56b347f9828e385bd04f4c73",
      "bea0250b1d7d45519e1335249b445a55",
      "8992eaf875a94ebaae93f2ae5940ac25",
      "828d7b1ba52f48bbae80dbdf8c3537ae",
      "aeb2de456f764b4dacafab63e6158ebb",
      "5267577b072344ffb885380a376aaf90"
     ]
    },
    "id": "RFjaY4Pdvbfy",
    "outputId": "068493e5-22da-4cf1-8a2d-e4bdb2c2957d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae032f7a014a406ab2581f795c6cc70d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457331b99edc45929a5d078293f22cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249592783a94ba1ab68a4aa3c6fc9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6c62f850cf4d0a941af1f69fbb5e5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful assistant<|end|><|user|> Tell a light-hearted joke for a room of Data Scientists<|end|><|assistant|> Why did the data scientist break up with the algorithm?\n",
      "\n",
      "Because it was always calculating the wrong variables in their relationship!<|end|>\n"
     ]
    }
   ],
   "source": [
    "generate(PHI3, messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxZQmZDCe4Jf"
   },
   "source": [
    "## Accessing Gemma from Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653,
     "referenced_widgets": [
      "b651755fe2a9402e94493dafc65ba22e",
      "a100c17a31f24c9fa4f4422456fc6c25",
      "30019d98cc7e48d58d58d1224fa0624a",
      "71c85b89580e4c28b291e9840b372462",
      "76e0634de5a54c568146875d5569467f",
      "bf7fe82eefc743eb98e37b03469731d6",
      "595faa2cf2894079b0fd2e2ae910a848",
      "f0c3f0a3d47c441b8d83dbdc068b453f",
      "2df05d5519d34953bb5b9a160d7ef015",
      "7532d6f9761946aa8e20e1f3a2876d87",
      "f892268eeec24c02b9323c382cb25981",
      "6740effcf5904b0fa27e8a9511b311eb",
      "880d34b54cd1413aab354a9cc376f5e2",
      "479227ca80e441be9ce15e6f4b60a02d",
      "4691a3de67be4050a289c1667a583f51",
      "1db609bdcbf84af0a2978543dba8631b",
      "4c818e37f9ee4d37ba6c84b1036ec406",
      "ef3578517b0b433ca380f122422f860d",
      "1f8c9b4091a842b19a419443b1a0de73",
      "d9719b71e23b48388b1c4feff5e55b8b",
      "82470a1f52194fbf9d853c3ad10cdf78",
      "84a36bef896a4b5f88e528a1980e716b",
      "69bd5169453a49368b2d154fc57cec1f",
      "b846938904cc47b0810e2d50608b568b",
      "51d6ff7db60e4bcb9f545307c38f65ca",
      "c0119b90a3c046f185e075744527a5da",
      "75d0d78b8aa245c2b9d0c7144d54685c",
      "62838e881c7444759899d91a61d2b37d",
      "f274f266f60442139c99d2277026bc94",
      "9e3aa47ac6a344c9a5bd0775864642dc",
      "e2208feac8514f69b8014a267ce40edd",
      "aebd0d310b0f443b86ef04ce1ebcee39",
      "54eca1e971ec486bbf77d05fc8f07ab5",
      "9009dee6b58547ea83038f615cca1b4d",
      "742f171894e845f6bf8c6cc0cea45de1",
      "4729503877b1441ba12251d253e9dcea",
      "1599a6e5394146cea4e48e7c797b4fa8",
      "39ef571f3c524a91998866e777e1e2e2",
      "8611eca37c1141e5b3e6540becfa2c7b",
      "e8b6989c4000467ebce5f8962f35ad48",
      "aab27f59b16d4f4bb14a6211d60603ac",
      "ee8fa467703d4cffafdcab4361038708",
      "e7e204a3b0054d8ebea6316dbec3b868",
      "6e2dc8b80b46481cac3d971555050013",
      "5f280e573b764c6c985e32b53e9c01f9",
      "47749ca7aa54434ca90517fb3b1ceed0",
      "2c54de82f12d4d6787aa924e2b1d2b8c",
      "91d9c5e725af474ebbc14567c6abed76",
      "44cc4f88f5dd4c469080a22b40c040d8",
      "786c0638c51846178a31342ef37aa6d3",
      "e058fb80a78c484b8f9ce28f737a85ef",
      "4ea0d73dfc044e489af61dab24b4a22e",
      "21f18f5d8bc843778325a0bbdd664698",
      "8e66a5168819486d882ce2289720a5aa",
      "8f77d02086b34f62b20fcbdf5d265d3f",
      "005c9153ebc64b9eb2832b370a252ee7",
      "19cc155a9a3e4a49a45d72801592362a",
      "c6ed983871814636bde3c45dd7c17499",
      "c300cf91442c483aa551e0996a71af16",
      "145e195c975247d291d7e18504b83969",
      "a70807d57c334162825b6a59f10372bc",
      "74d2dccdb4684c8ca6f3fa6502c7093c",
      "c7f8588bd66e4140928f891db0bfb8e4",
      "431116d29ab14f7ebf1f608ad0510622",
      "99e07141b2124613a6baecb182123095",
      "54dd1caa600b460ebb062c67449fb818",
      "c5a224ba9c01448db08afe9d6cb31ef6",
      "23c8162a18f5478c8104b4ef8791f732",
      "723ec4c657784b569e3ab6350d4e891b",
      "24849d97a26b42b9b0f34c600e97801d",
      "ccd5da9388784d0cabb3a28c7fbf1073",
      "1b943cfd13dc4b1faab2676114ab824f",
      "0971419678cc47fd87859ba33bab6245",
      "9bcef778c8c549bebdd315cecbebd9c5",
      "0947d9794df64d1b8e1d7e7bbb023bc1",
      "7d8460ab58ca4093883b9cda37b227d5",
      "b85240203d7643838458890a8cc2450a",
      "bf377ebbd06a454b9b0f14d5ac1a9e9e",
      "34e146f0e0814c88bb5ccfcb56da6ca6",
      "a7c79575fb304a479a13365e0123ad5f",
      "4fd3057e5e5c4f46b7ab1a6c3925a468",
      "0242038f46a64e9e80858ede38bb02de",
      "27945c238a1b4d1cad7750bd702378a6",
      "af056386c4714dcb88317f114323f5a2",
      "0023f4f9dcad42e0a56f14864bcae46c",
      "fee0418d251d4f29b4654522a045a5f6",
      "85f696c6666d414c97aca93963a80c81",
      "6ef1b136f83c425392796565292bb47b",
      "68b4140220ac4407bacc09f2cff90dd3",
      "dd8e52cdef1e4b148b6ed4e87693d8b6",
      "f79425c6eddf436ab60a387053a4d67b",
      "ed73bf247be841c09d6190c052030830",
      "a5ae685fadac463d99e8045bcfd8bd09",
      "f21ac259929947939c71efc3470be4b5",
      "0bcf0ae2235448eca24d510781a0aeaf",
      "edf9429a3b8048269f0e4bf96c2baff1",
      "de4bd11e56674ac8a024c0f8f005fd24",
      "835b21e9ac31495a8634105a008e39b6",
      "7fc170670f6245719344d699374ba604",
      "b9313b1f220847dab1603a3100cd367e",
      "203c391aa4574f868691ea26fe011ac6",
      "23aafa417f334ec5b9af089fb597c015",
      "2953e2b32a074c059d7ccbe85823bfa1",
      "e435f3079ed34ccd8a3ae7585e841b49",
      "f5827614c30e492ea504f66021e11a4d",
      "56aca2f7fa7f4f66b6b882f58205660e",
      "81076131b92c4f4ca641f1c958da0371",
      "48e3caa0c8af40b08affd253770d3932",
      "9c00871e418749a7bfc8b555fc278f45",
      "6e9386d20c314d48bee889b75d0dd8d1",
      "5f04240413334dbebaf063238e322210",
      "8f482ae63da341339c1a0043e495bd5f",
      "77ad9f4b20544ef38ced9d3009b7b077",
      "bb2ba779f1ad45f8a4f4d3fb9fa7651b",
      "9a4ba9c1be8d4ae592d87da370bdde11",
      "c44632ac6a0241b18cbad9368ec3fd38",
      "124c449f9b1d42269207809addfbfbde",
      "d67eb83e0a774f0ea682506728b7cca2",
      "80dbf5447f2343a0b7a0a2c2a76de475",
      "2907b018350445e2b0a53cdb707d7322",
      "7a5f6f3a0e3b48e89ea151417726816e"
     ]
    },
    "id": "q1JW41D-viGy",
    "outputId": "11ca7b2d-2006-415e-c90b-f4e08eb32028"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b651755fe2a9402e94493dafc65ba22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6740effcf5904b0fa27e8a9511b311eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bd5169453a49368b2d154fc57cec1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9009dee6b58547ea83038f615cca1b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f280e573b764c6c985e32b53e9c01f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005c9153ebc64b9eb2832b370a252ee7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a224ba9c01448db08afe9d6cb31ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf377ebbd06a454b9b0f14d5ac1a9e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68b4140220ac4407bacc09f2cff90dd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9313b1f220847dab1603a3100cd367e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f04240413334dbebaf063238e322210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Tell a light-hearted joke for a room of Data Scientists<end_of_turn>\n",
      "<start_of_turn>model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician? \n",
      "\n",
      "Because they had too many differences! 沽 \n",
      "\n",
      "---\n",
      "\n",
      "Let me know if you'd like to hear another joke! 沽 \n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
    "  ]\n",
    "generate(GEMMA2, messages)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
